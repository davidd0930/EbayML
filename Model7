# !pip install pandas
# !pip install torch
# !pip install numpy
# !pip install scikit-learn
# !pip install transformers
# !pip install logging
# !pip install notebook

import csv
import logging
import time
import numpy as np
import pandas as pd
import torch
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from tqdm import tqdm
from transformers import BertForTokenClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup
from transformers import BertTokenizerFast


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)



# Constants
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
LEARNING_RATE = 1e-3
EPSILON = 1e-8
EPOCHS = 10
BATCH_SIZE = 4
GRADIENT_ACCUMULATION_STEPS = 8
PATIENCE = 3
MAX_LENGTH = 256
MODEL_PATH = 'model_save2.pth'
PRETRAINED_MODEL_NAME = 'bert-base-german-cased'
DATA_PATH = 'Train_Tagged_Titles.tsv'

def encode_labels(title, labels, tokenizer, le, max_length=512):
    encodings = tokenizer(title, truncation=True, padding='max_length', max_length=max_length)

    # Initialize encoded labels with 'No Tag'
    encoded_labels = [le.transform(['No Tag'])[0]] * len(encodings['input_ids'])

    # The list of tokenized words in the title
    tokenized_words = tokenizer.tokenize(title)

    # Convert the tokenized words and their labels into a dictionary
    word_to_label = {word: label for word, label in zip(tokenized_words, labels)}

    # Get the tokens from the encoding
    tokens = tokenizer.convert_ids_to_tokens(encodings["input_ids"])

    # Assign labels to tokens
    for i, token in enumerate(tokens):
        if token in word_to_label:
            # Using only the first label if there are multiple labels for a token
            encoded_labels[i] = le.transform([word_to_label[token][0]])[0] if isinstance(word_to_label[token], list) else le.transform([word_to_label[token]])[0]
        elif token.startswith("##"):  # This token is a subword
            original_word = token[2:]
            if original_word in word_to_label:
                encoded_labels[i] = le.transform([word_to_label[original_word][0]])[0] if isinstance(word_to_label[original_word], list) else le.transform([word_to_label[original_word]])[0]
        elif token == "[CLS]" or token == "[SEP]":
            continue  # Skip special tokens
        else:
            encoded_labels[i] = le.transform(['No Tag'])[0]  # Label for unknown tokens

    encoded_labels = encoded_labels[:max_length]
    return encoded_labels


def load_and_process_data(filename):
    try:
        with open(filename, 'r', encoding='utf-8') as file:
            df = pd.read_csv(file, delimiter='\t', header=0, quoting=csv.QUOTE_NONE, engine='python')
    except Exception as e:
        logger.error(f"Failed to load data from file {filename} due to error: {str(e)}")
        raise e

    df.columns = ['Record Number', 'Title', 'Token', 'Tag']
    df['Tag'] = df['Tag'].fillna('No Tag').apply(
        lambda x: ['No Tag' if tag == 'No Tag' else tag for tag in x.split()])
    df['Tag'] = df['Tag'].apply(lambda x: x if 'No Tag' in x else x + ['No Tag'])
    grouped = df.groupby('Record Number').agg({'Token': lambda x: list(x), 'Tag': lambda x: list(x), 'Title': 'first'}).reset_index()
    return train_test_split(grouped, test_size=0.2, random_state=42, shuffle=True), df

def save_model(model, path):
    try:
        torch.save(model.state_dict(), path)
    except Exception as e:
        logger.error(f"Failed to save model due to error: {str(e)}")
        raise e


def testModel(data_file, model_address, device):
    num_labels = 40
    batch_size = 16

    model = BertForTokenClassification.from_pretrained(
        PRETRAINED_MODEL_NAME,
        num_labels=num_labels,
        # gradient_checkpointing=True,
    )

    model.load_state_dict(torch.load(model_address, map_location=device))
    model = model.to(device)

    tokenizer = BertTokenizerFast.from_pretrained('bert-base-german-cased')

    df = pd.read_csv(data_file, delimiter='\t', header=0, quoting=csv.QUOTE_NONE, engine='python')
    df.columns = ['Record Number', 'Title', 'Token', 'Tag']
    df['Tag'] = df['Tag'].fillna('No Tag').apply(
        lambda x: ['No Tag' if tag == 'No Tag' else tag for tag in x.split()])

    le = LabelEncoder()
    all_labels = list(df['Tag'].values)
    flattened_labels = [item for sublist in all_labels for item in sublist]
    le.fit(flattened_labels)

    grouped = df.groupby('Record Number').agg(
        {'Token': lambda x: list(x), 'Tag': lambda x: list(x), 'Title': 'first'}).reset_index()

    titles = grouped['Title'].values
    max_length = 512
    input_ids = []
    attention_masks = []

    for title in titles:
        encodings = tokenizer(title, add_special_tokens=True, max_length=max_length, padding='max_length',
                              truncation=True)
        input_ids.append(encodings['input_ids'])
        attention_masks.append(encodings['attention_mask'])

    input_ids = torch.tensor(input_ids).to(device)
    attention_masks = torch.tensor(attention_masks).to(device)

    data_loader = DataLoader(TensorDataset(input_ids, attention_masks), batch_size=batch_size)

    model = model.eval()
    predictions = []
    with torch.no_grad():
        for batch in data_loader:
            batch = tuple(t.to(device) for t in batch)
            b_input_ids, b_input_mask = batch

            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)
            logits = outputs[0]
            batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()

            for mask, pred in zip(b_input_mask.cpu().numpy(), batch_predictions):
                predictions.append(pred[mask == 1])

    flattened_predictions = [item for sublist in predictions for item in sublist]
    predicted_labels = le.inverse_transform(flattened_predictions)


    for title, labels in zip(titles, np.array_split(predicted_labels, len(titles))):
        print(f'Title: {title}, Predicted Tags: {labels.tolist()}')
        time.sleep(1)


def main():
    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    logger.info(f"Using device: {device}")
    (train_data, val_data), df = load_and_process_data(DATA_PATH)

    le = LabelEncoder()
    # all_labels = [['No Tag']] + list(df['Tag'].values)
    all_labels = list(df['Tag'].values)
    flattened_labels = [item for sublist in all_labels for item in sublist]
    le.fit(flattened_labels)

    num_labels = len(list(le.classes_))
    classes = np.unique(flattened_labels)
    class_weights = compute_class_weight('balanced', classes=classes, y=flattened_labels)
    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
    loss_function = torch.nn.CrossEntropyLoss(weight=class_weights)

    try:
        model = BertForTokenClassification.from_pretrained(
            PRETRAINED_MODEL_NAME,
            num_labels=num_labels,
        )
    except Exception as e:
        logger.error(f"Failed to load pretrained model due to error: {str(e)}")
        raise e

    if torch.cuda.device_count() > 1:
        logger.info(f"Let's use {torch.cuda.device_count()} GPUs!")
        model = torch.nn.DataParallel(model)

    model = model.to(device)
    tokenizer = BertTokenizerFast.from_pretrained(PRETRAINED_MODEL_NAME)

    input_ids = []
    attention_masks = []
    encoded_labels = []
    rowy = 0
    for _, row in train_data.iterrows():
        title = row['Title']
        encodings = tokenizer(title, truncation=True, padding='max_length', max_length=512)
        input_ids.append(encodings['input_ids'])
        attention_masks.append(encodings['attention_mask'])
        encoded_label = encode_labels(row['Title'], row['Tag'], tokenizer, le)
        rowy += 1
        if encoded_label:  # Check if list is not empty
            encoded_labels.append(encoded_label)

    input_ids = torch.tensor(input_ids, dtype=torch.long).to(device)
    attention_masks = torch.tensor(attention_masks, dtype=torch.long).to(device)
    encoded_labels = torch.tensor(encoded_labels, dtype=torch.long).to(device)

    train_inputs, val_inputs, train_labels, val_labels, train_masks, val_masks = train_test_split(input_ids,
                                                                                                  encoded_labels,
                                                                                                  attention_masks,
                                                                                                  random_state=42,
                                                                                                  shuffle=True,
                                                                                                  test_size=0.1)

    batch_size = BATCH_SIZE
    gradient_accumulation_steps = GRADIENT_ACCUMULATION_STEPS

    train_data = TensorDataset(train_inputs, train_masks, train_labels)
    train_sampler = RandomSampler(train_data)
    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

    val_data = TensorDataset(val_inputs, val_masks, val_labels)
    val_sampler = SequentialSampler(val_data)
    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

    total_steps = int(len(train_dataloader) * EPOCHS / gradient_accumulation_steps)
    optimizer = torch.optim.AdamW(model.parameters(), lr= LEARNING_RATE, eps= EPSILON)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

    model_save_path = MODEL_PATH
    best_val_loss = float('inf')

    n_epochs_no_improvement = 0
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0

        train_iterator = tqdm(train_dataloader, desc="Training", total=len(train_dataloader))

        optimizer.zero_grad()  # Initialize gradients
        for step, batch in enumerate(train_iterator):
            b_input_ids = batch[0].to(device)
            b_input_mask = batch[1].to(device)
            b_labels = batch[2].to(device)

            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
            loss = outputs[0]
            loss = loss / gradient_accumulation_steps  # Normalize our loss (if averaged)
            loss.backward()
            total_loss += loss.item()

            if (step + 1) % gradient_accumulation_steps == 0:  # Wait for several backward steps
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping
                optimizer.step()
                scheduler.step()
                model.zero_grad()

        avg_train_loss = total_loss / len(train_dataloader)
        print("Average train loss: {}".format(avg_train_loss))
        torch.save(model.state_dict(), model_save_path)

        model.eval()
        eval_loss, eval_accuracy = 0, 0
        predictions, true_labels = [], []

        validation_iterator = tqdm(val_dataloader, desc="Validation", total=len(val_dataloader))

        for batch in validation_iterator:
            batch = tuple(t.to(device) for t in batch)
            b_input_ids, b_input_mask, b_labels = batch

            with torch.no_grad():
                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
            logits = outputs.logits.detach().to(device)  # move to same device as label_ids
            label_ids = b_labels.to(device)  # make sure is on the same device
            tmp_eval_loss = loss_function(logits.view(-1, num_labels), label_ids.view(-1))
            eval_loss += tmp_eval_loss.item()
            logits_cpu = logits.cpu()
            predictions.extend([list(p) for p in np.argmax(logits_cpu.detach().numpy(), axis=2)])
            true_labels.extend(label_ids)

        eval_loss /= len(val_dataloader)
        print(f"Validation loss: {eval_loss}")
        if eval_loss < best_val_loss:  # You can modify the tolerance level
            torch.save(model.state_dict(), model_save_path)
            best_val_loss = eval_loss
            n_epochs_no_improvement = 0  # Reset the counter when the model improves
        else:
            n_epochs_no_improvement += 1  # Increment the counter when the model does not improve

        if n_epochs_no_improvement >= PATIENCE:
            print(f'Early stopping: No improvement for {n_epochs_no_improvement} epochs')
            break

        testModel('Train_Tagged_Titles.tsv', MODEL_PATH, device)

        flat_true_labels = [item.cpu() for sublist in true_labels for item in sublist]
        flat_predictions = [item.cpu() for sublist in predictions for item in sublist]

        print(classification_report(flat_true_labels, flat_predictions, zero_division=1))



if __name__ == '__main__':
    main()
