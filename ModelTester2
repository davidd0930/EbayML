from transformers import BertForTokenClassification, BertTokenizer
import torch
import pandas as pd
import csv
from sklearn.preprocessing import LabelEncoder

def predict_tags(data_file, model_address):
    # Initialize a new model with the same architecture
    model = BertForTokenClassification.from_pretrained(
        'bert-base-uncased',
        gradient_checkpointing=True,
    )

    # Load the state dictionary
    model.load_state_dict(torch.load(model_address))

    # Load tokenizer
    tokenizer = BertTokenizer.from_pretrained('bert-german-base-cased')

    # Load and process data
    with open(data_file, 'r', encoding='utf-8') as file:
        df = pd.read_csv(file, delimiter='\t', header=0, quoting=csv.QUOTE_NONE, engine='python')
    df.columns = ['Record Number', 'Title', 'Token', 'Tag']
    df['Tag'] = df['Tag'].fillna('No Tag').apply(lambda x: ['No Tag' if tag == 'No Tag' else tag for tag in x.split()])

    # Create a LabelEncoder
    le = LabelEncoder()
    all_labels = [['No Tag']] + list(df['Tag'].values)
    flattened_labels = [item for sublist in all_labels for item in sublist]
    le.fit(flattened_labels)

    # Prepare data for prediction
    titles = df['Title'].values
    input_ids = [tokenizer.encode(title, add_special_tokens=True) for title in titles]
    input_ids = torch.tensor(input_ids)  # Create torch tensors

    # Get model predictions
    model = model.eval()
    with torch.no_grad():
        outputs = model(input_ids)
    logits = outputs[0]

    # Convert logits to predicted class IDs
    predictions = torch.argmax(logits, dim=-1)

    # Convert class IDs back to original class labels
    predicted_labels = le.inverse_transform(predictions)

    # Print out titles with their predicted tags
    for title, label in zip(titles, predicted_labels):
        print(f'Title: {title}, Predicted Tag: {label}')

predict_tags('Train_Tagged_Titles.tsv', 'model_save.pth')
